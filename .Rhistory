part_index_1 <- caret::createDataPartition(credit_hot$STATUS,
times=1,
p = 0.70,
groups=1,
list=FALSE)
train <- credit_hot[part_index_1, ]
tune_and_test <- credit_hot[-part_index_1, ]
tune_and_test_index <- createDataPartition(tune_and_test$STATUS,
p = .5,
list = FALSE,
times = 1)
tune <- tune_and_test[tune_and_test_index, ]#aka, validation set
test <- tune_and_test[-tune_and_test_index, ]#final testing set
dim(train)
dim(test)# these will be slightly off because the data set isn't perfectly even
#buts its not a issue.
dim(tune)
#View(train)
```
```{r}
#check the prevalence
table(credit$STATUS)
(prevalence <- table(credit$STATUS)[[2]]/length(credit$STATUS))
table(credit$STATUS)
#check the prevalence
table(credit$STATUS)
(prevalence <- table(credit$STATUS)[[2]]/length(credit$STATUS))
table(credit$STATUS)
Calculate the initial mtry level
```{r}
mytry_tune <- function(x){
xx <- dim(x)[2]-1
sqrt(xx)
}
mytry_tune(credit_hot)
View(credit_hot)
View(train$STATUS)
str(train)
```
Run the initial RF model with 500 trees
```{r, cache=TRUE}
credit_RF = randomForest(STATUS ~ .,          #<- Formula: response variable ~ predictors.
#   The period means 'use all other variables in the data'.
train,     #<- A data frame with the variables to be used.
#y = NULL,           #<- A response vector. This is unnecessary because we're specifying a response formula.
#subset = NULL,      #<- This is unnecessary because we're using all the rows in the training data set.
#xtest = NULL,       #<- This is already defined in the formula by the ".".
#ytest = NULL,       #<- This is already defined in the formula by "PREGNANT".
ntree = 200,        #<- Number of trees to grow. This should not be set to too small a number, to ensure that every input row gets classified at least a few times.
mtry = 7,            #<- Number of variables randomly sampled as candidates at each split. Default number for classification is sqrt(# of variables). Default number for regression is (# of variables / 3).
replace = TRUE,      #<- Should sampled data points be replaced.
#classwt = NULL,     #<- Priors of the classes. Use this if you want to specify what proportion of the data SHOULD be in each class. This is relevant if your sample data is not completely representative of the actual population
#strata = NULL,      #<- Not necessary for our purpose here.
sampsize = 200,      #<- Size of sample to draw each time.
nodesize = 5,        #<- Minimum numbers of data points in terminal nodes.
#maxnodes = NULL,    #<- Limits the number of maximum splits.
importance = TRUE,   #<- Should importance of predictors be assessed?
#localImp = FALSE,   #<- Should casewise importance measure be computed? (Setting this to TRUE will override importance.)
proximity = FALSE,    #<- Should a proximity measure between rows be calculated?
norm.votes = TRUE,   #<- If TRUE (default), the final result of votes are expressed as fractions. If FALSE, raw vote counts are returned (useful for combining results from different runs).
do.trace = TRUE,     #<- If set to TRUE, give a more verbose output as randomForest is run.
keep.forest = TRUE,  #<- If set to FALSE, the forest will not be retained in the output object. If xtest is given, defaults to FALSE.
keep.inbag = TRUE)   #<- Should an n by ntree matrix be returned that keeps track of which samples are in-bag in which trees?
credit_RF
r
r
r
#url <- "https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data"
data <- read_csv("application_record.csv")
data2 <- read_csv("credit_record.csv")
count(data2)
count(data)
credit <-merge(data,data2)
count(credit)
credit$STATUS <- fct_collapse(credit$STATUS, paid=c("C","X"), late=c("0","1","5","3","2","4"))
credit$OCCUPATION_TYPE <- replace(credit$OCCUPATION_TYPE, is.na(credit$OCCUPATION_TYPE), "Unknown")
credit <- na.omit(credit)
count(credit)
#Drop unneeded columns
credit = select(credit, -c("ID","MONTHS_BALANCE","AMT_INCOME_TOTAL","DAYS_EMPLOYED","DAYS_BIRTH"))
#Convert columns to factors
factors <- c("CODE_GENDER","FLAG_OWN_CAR","FLAG_OWN_REALTY",
"NAME_INCOME_TYPE", "NAME_EDUCATION_TYPE","NAME_FAMILY_STATUS",
"NAME_HOUSING_TYPE","FLAG_MOBIL","FLAG_WORK_PHONE","FLAG_PHONE",
"FLAG_EMAIL","OCCUPATION_TYPE");
credit[,factors] <- lapply(credit[,factors], as.factor)
#one hot-encoding
credit_temp <- subset(credit, select = -STATUS)
credit_hot <- one_hot(as.data.table(credit_temp),cols = "auto",sparsifyNAs = TRUE,naCols = TRUE,dropCols = TRUE,dropUnusedLevels = TRUE)
credit_hot$STATUS = credit$STATUS
names(credit_hot) <- make.names(names(credit_hot))
#6 Split your data into test, tune, and train. (70/15/15)
part_index_1 <- caret::createDataPartition(credit_hot$STATUS,
times=1,
p = 0.70,
groups=1,
list=FALSE)
train <- credit_hot[part_index_1, ]
tune_and_test <- credit_hot[-part_index_1, ]
tune_and_test_index <- createDataPartition(tune_and_test$STATUS,
p = .5,
list = FALSE,
times = 1)
tune <- tune_and_test[tune_and_test_index, ]#aka, validation set
test <- tune_and_test[-tune_and_test_index, ]#final testing set
dim(train)
dim(test)# these will be slightly off because the data set isn't perfectly even
#buts its not a issue.
dim(tune)
#View(train)
```
#check the prevalence
table(credit$STATUS)
(prevalence <- table(credit$STATUS)[[2]]/length(credit$STATUS))
table(credit$STATUS)
#check the prevalence
table(credit$STATUS)
(prevalence <- table(credit$STATUS)[[2]]/length(credit$STATUS))
table(credit$STATUS)
Calculate the initial mtry level
```{r}
mytry_tune <- function(x){
xx <- dim(x)[2]-1
sqrt(xx)
}
mytry_tune(credit_hot)
View(credit_hot)
View(train$STATUS)
str(train)
```
Run the initial RF model with 500 trees
```{r, cache=TRUE}
credit_RF = randomForest(STATUS ~ .,          #<- Formula: response variable ~ predictors.
#   The period means 'use all other variables in the data'.
train,     #<- A data frame with the variables to be used.
#y = NULL,           #<- A response vector. This is unnecessary because we're specifying a response formula.
#subset = NULL,      #<- This is unnecessary because we're using all the rows in the training data set.
#xtest = NULL,       #<- This is already defined in the formula by the ".".
#ytest = NULL,       #<- This is already defined in the formula by "PREGNANT".
ntree = 200,        #<- Number of trees to grow. This should not be set to too small a number, to ensure that every input row gets classified at least a few times.
mtry = 7,            #<- Number of variables randomly sampled as candidates at each split. Default number for classification is sqrt(# of variables). Default number for regression is (# of variables / 3).
replace = TRUE,      #<- Should sampled data points be replaced.
#classwt = NULL,     #<- Priors of the classes. Use this if you want to specify what proportion of the data SHOULD be in each class. This is relevant if your sample data is not completely representative of the actual population
#strata = NULL,      #<- Not necessary for our purpose here.
sampsize = 200,      #<- Size of sample to draw each time.
nodesize = 5,        #<- Minimum numbers of data points in terminal nodes.
#maxnodes = NULL,    #<- Limits the number of maximum splits.
importance = TRUE,   #<- Should importance of predictors be assessed?
#localImp = FALSE,   #<- Should casewise importance measure be computed? (Setting this to TRUE will override importance.)
proximity = FALSE,    #<- Should a proximity measure between rows be calculated?
norm.votes = TRUE,   #<- If TRUE (default), the final result of votes are expressed as fractions. If FALSE, raw vote counts are returned (useful for combining results from different runs).
do.trace = TRUE,     #<- If set to TRUE, give a more verbose output as randomForest is run.
keep.forest = TRUE,  #<- If set to FALSE, the forest will not be retained in the output object. If xtest is given, defaults to FALSE.
keep.inbag = TRUE)   #<- Should an n by ntree matrix be returned that keeps track of which samples are in-bag in which trees?
credit_RF = randomForest(STATUS ~ .,          #<- Formula: response variable ~ predictors.
#   The period means 'use all other variables in the data'.
train,     #<- A data frame with the variables to be used.
#y = NULL,           #<- A response vector. This is unnecessary because we're specifying a response formula.
#subset = NULL,      #<- This is unnecessary because we're using all the rows in the training data set.
#xtest = NULL,       #<- This is already defined in the formula by the ".".
#ytest = NULL,       #<- This is already defined in the formula by "PREGNANT".
ntree = 200,        #<- Number of trees to grow. This should not be set to too small a number, to ensure that every input row gets classified at least a few times.
mtry = 7,            #<- Number of variables randomly sampled as candidates at each split. Default number for classification is sqrt(# of variables). Default number for regression is (# of variables / 3).
replace = TRUE,      #<- Should sampled data points be replaced.
#classwt = NULL,     #<- Priors of the classes. Use this if you want to specify what proportion of the data SHOULD be in each class. This is relevant if your sample data is not completely representative of the actual population
#strata = NULL,      #<- Not necessary for our purpose here.
sampsize = 200,      #<- Size of sample to draw each time.
nodesize = 5,        #<- Minimum numbers of data points in terminal nodes.
#maxnodes = NULL,    #<- Limits the number of maximum splits.
importance = TRUE,   #<- Should importance of predictors be assessed?
#localImp = FALSE,   #<- Should casewise importance measure be computed? (Setting this to TRUE will override importance.)
proximity = FALSE,    #<- Should a proximity measure between rows be calculated?
norm.votes = TRUE,   #<- If TRUE (default), the final result of votes are expressed as fractions. If FALSE, raw vote counts are returned (useful for combining results from different runs).
do.trace = TRUE,     #<- If set to TRUE, give a more verbose output as randomForest is run.
keep.forest = TRUE,  #<- If set to FALSE, the forest will not be retained in the output object. If xtest is given, defaults to FALSE.
keep.inbag = TRUE)   #<- Should an n by ntree matrix be returned that keeps track of which samples are in-bag in which trees?
clear
mytry_tune <- function(x){
xx <- dim(x)[2]-1
sqrt(xx)
}
mytry_tune(credit_hot)
View(credit_hot)
credit_RF = randomForest(STATUS ~ .,          #<- Formula: response variable ~ predictors.
#   The period means 'use all other variables in the data'.
train,     #<- A data frame with the variables to be used.
#y = NULL,           #<- A response vector. This is unnecessary because we're specifying a response formula.
#subset = NULL,      #<- This is unnecessary because we're using all the rows in the training data set.
#xtest = NULL,       #<- This is already defined in the formula by the ".".
#ytest = NULL,       #<- This is already defined in the formula by "PREGNANT".
ntree = 200,        #<- Number of trees to grow. This should not be set to too small a number, to ensure that every input row gets classified at least a few times.
mtry = 7,            #<- Number of variables randomly sampled as candidates at each split. Default number for classification is sqrt(# of variables). Default number for regression is (# of variables / 3).
replace = TRUE,      #<- Should sampled data points be replaced.
#classwt = NULL,     #<- Priors of the classes. Use this if you want to specify what proportion of the data SHOULD be in each class. This is relevant if your sample data is not completely representative of the actual population
#strata = NULL,      #<- Not necessary for our purpose here.
sampsize = 200,      #<- Size of sample to draw each time.
nodesize = 5,        #<- Minimum numbers of data points in terminal nodes.
#maxnodes = NULL,    #<- Limits the number of maximum splits.
importance = TRUE,   #<- Should importance of predictors be assessed?
#localImp = FALSE,   #<- Should casewise importance measure be computed? (Setting this to TRUE will override importance.)
proximity = FALSE,    #<- Should a proximity measure between rows be calculated?
norm.votes = TRUE,   #<- If TRUE (default), the final result of votes are expressed as fractions. If FALSE, raw vote counts are returned (useful for combining results from different runs).
do.trace = TRUE,     #<- If set to TRUE, give a more verbose output as randomForest is run.
keep.forest = TRUE,  #<- If set to FALSE, the forest will not be retained in the output object. If xtest is given, defaults to FALSE.
keep.inbag = TRUE)   #<- Should an n by ntree matrix be returned that keeps track of which samples are in-bag in which trees?
credit_RF
#Best results achieved with "census_RF_more_tuning" model with a validation accuracy of 0.8624
credit_predict_test = predict(credit_RF,      #<- a randomForest model
test,      #<- the test data set to use
type ="response",   #<- what results to produce, see the help menu for the options
predict.all = FALSE,  #<- should the predictions of all trees be kept?
proximity = FALSE)    #<- should proximity measures be computed
(credit_eval <- confusionMatrix(as.factor(credit_predict_test),
as.factor(test$STATUS),
dnn=c("Prediction", "Actual"),
mode = "sens_spec"))
(credit_eval <- confusionMatrix(as.factor(credit_predict_test),
as.factor(test$STATUS),
dnn=c("Prediction", "Actual"),
mode = "sens_spec", positive="paid"))
View(as.data.frame(importance(credit_RF, type = 2, scale = TRUE)))
#url <- "https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data"
data <- read_csv("application_record.csv")
data2 <- read_csv("credit_record.csv")
count(data2)
count(data)
credit <-merge(data,data2)
count(credit)
credit$STATUS <- fct_collapse(credit$STATUS, paid=c("C","X"), late=c("0","1","5","3","2","4"))
credit$OCCUPATION_TYPE <- replace(credit$OCCUPATION_TYPE, is.na(credit$OCCUPATION_TYPE), "Unknown")
credit <- na.omit(credit)
count(credit)
View(credit)
str(credit)
#Drop unneeded columns
credit = select(credit, -c("ID"))
#Convert columns to factors
factors <- c("CODE_GENDER","FLAG_OWN_CAR","FLAG_OWN_REALTY",
"NAME_INCOME_TYPE", "NAME_EDUCATION_TYPE","NAME_FAMILY_STATUS",
"NAME_HOUSING_TYPE","FLAG_MOBIL","FLAG_WORK_PHONE","FLAG_PHONE",
"FLAG_EMAIL","OCCUPATION_TYPE");
credit[,factors] <- lapply(credit[,factors], as.factor)
#one hot-encoding
credit_temp <- subset(credit, select = -STATUS)
credit_hot <- one_hot(as.data.table(credit_temp),cols = "auto",sparsifyNAs = TRUE,naCols = TRUE,dropCols = TRUE,dropUnusedLevels = TRUE)
credit_hot$STATUS = credit$STATUS
names(credit_hot) <- make.names(names(credit_hot))
View(credit_hot)
#6 Split your data into test, tune, and train. (70/15/15)
part_index_1 <- caret::createDataPartition(credit_hot$STATUS,
times=1,
p = 0.70,
groups=1,
list=FALSE)
train <- credit_hot[part_index_1, ]
tune_and_test <- credit_hot[-part_index_1, ]
tune_and_test_index <- createDataPartition(tune_and_test$STATUS,
p = .5,
list = FALSE,
times = 1)
tune <- tune_and_test[tune_and_test_index, ]#aka, validation set
test <- tune_and_test[-tune_and_test_index, ]#final testing set
dim(train)
dim(test)# these will be slightly off because the data set isn't perfectly even
#buts its not a issue.
dim(tune)
#check the prevalence
table(credit$STATUS)
(prevalence <- table(credit$STATUS)[[2]]/length(credit$STATUS))
mytry_tune <- function(x){
xx <- dim(x)[2]-1
sqrt(xx)
}
mytry_tune(credit_hot)
View(credit_hot)
credit_RF = randomForest(STATUS ~ .,          #<- Formula: response variable ~ predictors.
#   The period means 'use all other variables in the data'.
train,     #<- A data frame with the variables to be used.
#y = NULL,           #<- A response vector. This is unnecessary because we're specifying a response formula.
#subset = NULL,      #<- This is unnecessary because we're using all the rows in the training data set.
#xtest = NULL,       #<- This is already defined in the formula by the ".".
#ytest = NULL,       #<- This is already defined in the formula by "PREGNANT".
ntree = 200,        #<- Number of trees to grow. This should not be set to too small a number, to ensure that every input row gets classified at least a few times.
mtry = 7,            #<- Number of variables randomly sampled as candidates at each split. Default number for classification is sqrt(# of variables). Default number for regression is (# of variables / 3).
replace = TRUE,      #<- Should sampled data points be replaced.
#classwt = NULL,     #<- Priors of the classes. Use this if you want to specify what proportion of the data SHOULD be in each class. This is relevant if your sample data is not completely representative of the actual population
#strata = NULL,      #<- Not necessary for our purpose here.
sampsize = 200,      #<- Size of sample to draw each time.
nodesize = 5,        #<- Minimum numbers of data points in terminal nodes.
#maxnodes = NULL,    #<- Limits the number of maximum splits.
importance = TRUE,   #<- Should importance of predictors be assessed?
#localImp = FALSE,   #<- Should casewise importance measure be computed? (Setting this to TRUE will override importance.)
proximity = FALSE,    #<- Should a proximity measure between rows be calculated?
norm.votes = TRUE,   #<- If TRUE (default), the final result of votes are expressed as fractions. If FALSE, raw vote counts are returned (useful for combining results from different runs).
do.trace = TRUE,     #<- If set to TRUE, give a more verbose output as randomForest is run.
keep.forest = TRUE,  #<- If set to FALSE, the forest will not be retained in the output object. If xtest is given, defaults to FALSE.
keep.inbag = TRUE)   #<- Should an n by ntree matrix be returned that keeps track of which samples are in-bag in which trees?
View(as.data.frame(importance(credit_RF, type = 2, scale = TRUE)))
unique(credit$NAME_EDUCATION_TYPE)
#url <- "https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data"
data <- read_csv("application_record.csv")
data2 <- read_csv("credit_record.csv")
count(data2)
count(data)
credit <-merge(data,data2)
count(credit)
credit$STATUS <- fct_collapse(credit$STATUS, paid=c("C","X"), late=c("0","1","5","3","2","4"))
credit$OCCUPATION_TYPE <- replace(credit$OCCUPATION_TYPE, is.na(credit$OCCUPATION_TYPE), "Unknown")
credit <- na.omit(credit)
count(credit)
View(credit)
str(credit)
#Drop unneeded columns
credit = select(credit, -c("ID","FLAG_MOBIL","NAME_EDUCATION_TYPE","OCCUPATION_TYPE"))
#Convert columns to factors
factors <- c("CODE_GENDER","FLAG_OWN_CAR","FLAG_OWN_REALTY",
"NAME_INCOME_TYPE","NAME_FAMILY_STATUS",
"NAME_HOUSING_TYPE","FLAG_WORK_PHONE","FLAG_PHONE",
"FLAG_EMAIL");
credit[,factors] <- lapply(credit[,factors], as.factor)
#one hot-encoding
credit_temp <- subset(credit, select = -STATUS)
credit_hot <- one_hot(as.data.table(credit_temp),cols = "auto",sparsifyNAs = TRUE,naCols = TRUE,dropCols = TRUE,dropUnusedLevels = TRUE)
credit_hot$STATUS = credit$STATUS
names(credit_hot) <- make.names(names(credit_hot))
View(credit_hot)
#6 Split your data into test, tune, and train. (70/15/15)
part_index_1 <- caret::createDataPartition(credit_hot$STATUS,
times=1,
p = 0.70,
groups=1,
list=FALSE)
train <- credit_hot[part_index_1, ]
tune_and_test <- credit_hot[-part_index_1, ]
tune_and_test_index <- createDataPartition(tune_and_test$STATUS,
p = .5,
list = FALSE,
times = 1)
tune <- tune_and_test[tune_and_test_index, ]#aka, validation set
test <- tune_and_test[-tune_and_test_index, ]#final testing set
dim(train)
dim(test)# these will be slightly off because the data set isn't perfectly even
#buts its not a issue.
dim(tune)
#check the prevalence
table(credit$STATUS)
(prevalence <- table(credit$STATUS)[[2]]/length(credit$STATUS))
mytry_tune <- function(x){
xx <- dim(x)[2]-1
sqrt(xx)
}
mytry_tune(credit_hot)
View(credit_hot)
set.seed(1)
credit_RF = randomForest(STATUS ~ .,          #<- Formula: response variable ~ predictors.
#   The period means 'use all other variables in the data'.
train,     #<- A data frame with the variables to be used.
#y = NULL,           #<- A response vector. This is unnecessary because we're specifying a response formula.
#subset = NULL,      #<- This is unnecessary because we're using all the rows in the training data set.
#xtest = NULL,       #<- This is already defined in the formula by the ".".
#ytest = NULL,       #<- This is already defined in the formula by "PREGNANT".
ntree = 200,        #<- Number of trees to grow. This should not be set to too small a number, to ensure that every input row gets classified at least a few times.
mtry = 7,            #<- Number of variables randomly sampled as candidates at each split. Default number for classification is sqrt(# of variables). Default number for regression is (# of variables / 3).
replace = TRUE,      #<- Should sampled data points be replaced.
#classwt = NULL,     #<- Priors of the classes. Use this if you want to specify what proportion of the data SHOULD be in each class. This is relevant if your sample data is not completely representative of the actual population
#strata = NULL,      #<- Not necessary for our purpose here.
sampsize = 200,      #<- Size of sample to draw each time.
nodesize = 5,        #<- Minimum numbers of data points in terminal nodes.
#maxnodes = NULL,    #<- Limits the number of maximum splits.
importance = TRUE,   #<- Should importance of predictors be assessed?
#localImp = FALSE,   #<- Should casewise importance measure be computed? (Setting this to TRUE will override importance.)
proximity = FALSE,    #<- Should a proximity measure between rows be calculated?
norm.votes = TRUE,   #<- If TRUE (default), the final result of votes are expressed as fractions. If FALSE, raw vote counts are returned (useful for combining results from different runs).
do.trace = TRUE,     #<- If set to TRUE, give a more verbose output as randomForest is run.
keep.forest = TRUE,  #<- If set to FALSE, the forest will not be retained in the output object. If xtest is given, defaults to FALSE.
keep.inbag = TRUE)   #<- Should an n by ntree matrix be returned that keeps track of which samples are in-bag in which trees?
set.seed(1)
credit_RF = randomForest(STATUS ~ .,          #<- Formula: response variable ~ predictors.
#   The period means 'use all other variables in the data'.
train,     #<- A data frame with the variables to be used.
#y = NULL,           #<- A response vector. This is unnecessary because we're specifying a response formula.
#subset = NULL,      #<- This is unnecessary because we're using all the rows in the training data set.
#xtest = NULL,       #<- This is already defined in the formula by the ".".
#ytest = NULL,       #<- This is already defined in the formula by "PREGNANT".
ntree = 200,        #<- Number of trees to grow. This should not be set to too small a number, to ensure that every input row gets classified at least a few times.
mtry = 6,            #<- Number of variables randomly sampled as candidates at each split. Default number for classification is sqrt(# of variables). Default number for regression is (# of variables / 3).
replace = TRUE,      #<- Should sampled data points be replaced.
#classwt = NULL,     #<- Priors of the classes. Use this if you want to specify what proportion of the data SHOULD be in each class. This is relevant if your sample data is not completely representative of the actual population
#strata = NULL,      #<- Not necessary for our purpose here.
sampsize = 200,      #<- Size of sample to draw each time.
nodesize = 5,        #<- Minimum numbers of data points in terminal nodes.
#maxnodes = NULL,    #<- Limits the number of maximum splits.
importance = TRUE,   #<- Should importance of predictors be assessed?
#localImp = FALSE,   #<- Should casewise importance measure be computed? (Setting this to TRUE will override importance.)
proximity = FALSE,    #<- Should a proximity measure between rows be calculated?
norm.votes = TRUE,   #<- If TRUE (default), the final result of votes are expressed as fractions. If FALSE, raw vote counts are returned (useful for combining results from different runs).
do.trace = TRUE,     #<- If set to TRUE, give a more verbose output as randomForest is run.
keep.forest = TRUE,  #<- If set to FALSE, the forest will not be retained in the output object. If xtest is given, defaults to FALSE.
keep.inbag = TRUE)   #<- Should an n by ntree matrix be returned that keeps track of which samples are in-bag in which trees?
credit_RF
View(as.data.frame(importance(credit_RF, type = 2, scale = TRUE)))
credit <- filter(credit, credit$STATUS!="X")
View(credit)
#url <- "https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data"
data <- read_csv("application_record.csv")
data2 <- read_csv("credit_record.csv")
count(data2)
count(data)
credit <-merge(data,data2)
count(credit)
credit <- filter(credit, credit$STATUS!="X")
View(credit)
unique(credit$STATUS)
credit$STATUS <- fct_collapse(credit$STATUS, paid=c("C"), late=c("0","1","5","3","2","4"))
credit$OCCUPATION_TYPE <- replace(credit$OCCUPATION_TYPE, is.na(credit$OCCUPATION_TYPE), "Unknown")
credit <- na.omit(credit)
count(credit)
View(credit)
str(credit)
#Drop unneeded columns
credit = select(credit, -c("ID","FLAG_MOBIL","NAME_EDUCATION_TYPE","OCCUPATION_TYPE"))
#Convert columns to factors
factors <- c("CODE_GENDER","FLAG_OWN_CAR","FLAG_OWN_REALTY",
"NAME_INCOME_TYPE","NAME_FAMILY_STATUS",
"NAME_HOUSING_TYPE","FLAG_WORK_PHONE","FLAG_PHONE",
"FLAG_EMAIL");
credit[,factors] <- lapply(credit[,factors], as.factor)
#one hot-encoding
credit_temp <- subset(credit, select = -STATUS)
credit_hot <- one_hot(as.data.table(credit_temp),cols = "auto",sparsifyNAs = TRUE,naCols = TRUE,dropCols = TRUE,dropUnusedLevels = TRUE)
credit_hot$STATUS = credit$STATUS
names(credit_hot) <- make.names(names(credit_hot))
View(credit_hot)
credit_hot$STATUS = credit$STATUS
names(credit_hot) <- make.names(names(credit_hot))
View(credit_hot)
Create test, tune and training sets
```{r}
#6 Split your data into test, tune, and train. (70/15/15)
part_index_1 <- caret::createDataPartition(credit_hot$STATUS,
times=1,
p = 0.70,
groups=1,
list=FALSE)
train <- credit_hot[part_index_1, ]
tune_and_test <- credit_hot[-part_index_1, ]
tune_and_test_index <- createDataPartition(tune_and_test$STATUS,
p = .5,
list = FALSE,
times = 1)
tune <- tune_and_test[tune_and_test_index, ]#aka, validation set
test <- tune_and_test[-tune_and_test_index, ]#final testing set
dim(train)
dim(test)# these will be slightly off because the data set isn't perfectly even
#buts its not a issue.
dim(tune)
#check the prevalence
table(credit$STATUS)
(prevalence <- table(credit$STATUS)[[2]]/length(credit$STATUS))
mytry_tune <- function(x){
xx <- dim(x)[2]-1
sqrt(xx)
}
mytry_tune(credit_hot)
View(credit_hot)
View(train$STATUS)
set.seed(1)
credit_RF = randomForest(STATUS ~ .,          #<- Formula: response variable ~ predictors.
#   The period means 'use all other variables in the data'.
train,     #<- A data frame with the variables to be used.
#y = NULL,           #<- A response vector. This is unnecessary because we're specifying a response formula.
#subset = NULL,      #<- This is unnecessary because we're using all the rows in the training data set.
#xtest = NULL,       #<- This is already defined in the formula by the ".".
#ytest = NULL,       #<- This is already defined in the formula by "PREGNANT".
ntree = 200,        #<- Number of trees to grow. This should not be set to too small a number, to ensure that every input row gets classified at least a few times.
mtry = 6,            #<- Number of variables randomly sampled as candidates at each split. Default number for classification is sqrt(# of variables). Default number for regression is (# of variables / 3).
replace = TRUE,      #<- Should sampled data points be replaced.
#classwt = NULL,     #<- Priors of the classes. Use this if you want to specify what proportion of the data SHOULD be in each class. This is relevant if your sample data is not completely representative of the actual population
#strata = NULL,      #<- Not necessary for our purpose here.
sampsize = 200,      #<- Size of sample to draw each time.
nodesize = 5,        #<- Minimum numbers of data points in terminal nodes.
#maxnodes = NULL,    #<- Limits the number of maximum splits.
importance = TRUE,   #<- Should importance of predictors be assessed?
#localImp = FALSE,   #<- Should casewise importance measure be computed? (Setting this to TRUE will override importance.)
proximity = FALSE,    #<- Should a proximity measure between rows be calculated?
norm.votes = TRUE,   #<- If TRUE (default), the final result of votes are expressed as fractions. If FALSE, raw vote counts are returned (useful for combining results from different runs).
do.trace = TRUE,     #<- If set to TRUE, give a more verbose output as randomForest is run.
keep.forest = TRUE,  #<- If set to FALSE, the forest will not be retained in the output object. If xtest is given, defaults to FALSE.
keep.inbag = TRUE)   #<- Should an n by ntree matrix be returned that keeps track of which samples are in-bag in which trees?
set.seed(1)
credit_RF = randomForest(STATUS ~ .,          #<- Formula: response variable ~ predictors.
#   The period means 'use all other variables in the data'.
train,     #<- A data frame with the variables to be used.
#y = NULL,           #<- A response vector. This is unnecessary because we're specifying a response formula.
#subset = NULL,      #<- This is unnecessary because we're using all the rows in the training data set.
#xtest = NULL,       #<- This is already defined in the formula by the ".".
#ytest = NULL,       #<- This is already defined in the formula by "PREGNANT".
ntree = 500,        #<- Number of trees to grow. This should not be set to too small a number, to ensure that every input row gets classified at least a few times.
mtry = 6,            #<- Number of variables randomly sampled as candidates at each split. Default number for classification is sqrt(# of variables). Default number for regression is (# of variables / 3).
replace = TRUE,      #<- Should sampled data points be replaced.
#classwt = NULL,     #<- Priors of the classes. Use this if you want to specify what proportion of the data SHOULD be in each class. This is relevant if your sample data is not completely representative of the actual population
#strata = NULL,      #<- Not necessary for our purpose here.
sampsize = 200,      #<- Size of sample to draw each time.
nodesize = 5,        #<- Minimum numbers of data points in terminal nodes.
#maxnodes = NULL,    #<- Limits the number of maximum splits.
importance = TRUE,   #<- Should importance of predictors be assessed?
#localImp = FALSE,   #<- Should casewise importance measure be computed? (Setting this to TRUE will override importance.)
proximity = FALSE,    #<- Should a proximity measure between rows be calculated?
norm.votes = TRUE,   #<- If TRUE (default), the final result of votes are expressed as fractions. If FALSE, raw vote counts are returned (useful for combining results from different runs).
do.trace = TRUE,     #<- If set to TRUE, give a more verbose output as randomForest is run.
keep.forest = TRUE,  #<- If set to FALSE, the forest will not be retained in the output object. If xtest is given, defaults to FALSE.
keep.inbag = TRUE)   #<- Should an n by ntree matrix be returned that keeps track of which samples are in-bag in which trees?
